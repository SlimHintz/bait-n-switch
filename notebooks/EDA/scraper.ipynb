{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/tjh/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "# Custom modules \n",
    "from modules import preprocessing as pp\n",
    "from modules import graph\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "# Extend stopwords (see analysis below)\n",
    "extension = {\n",
    "    'trumps',\n",
    "    'trump',\n",
    "    'obama',\n",
    "    'donald',\n",
    "    'new',\n",
    "    'u',\n",
    "    'tramp'\n",
    "}\n",
    "stop_words.update(extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first function just has to access a url and return a list of all headlines\n",
    "url = 'https://twitter.com/home'\n",
    "response = requests.get(url)\n",
    "\n",
    "def get_headlines(response_text, tags=['h1', 'h2', 'h3', 'h4']):\n",
    "    soup = BeautifulSoup(response_text, 'lxml')\n",
    "    headers = soup.find_all(tags)\n",
    "    return [header.text for header in headers]\n",
    "\n",
    "def clean_headlines(title, length):\n",
    "#     if len(title.split()) >= length:\n",
    "#         return None\n",
    "#     else:\n",
    "        # strip newline characters\n",
    "    title = title.replace(\"\\n\", \"\")\n",
    "    title = title.replace(\"\\t\", \"\")\n",
    "    title = pp.remove_non_ascii_chars(title)\n",
    "    title = pp.lower_case(title)\n",
    "    title = pp.remove_contractions(title)\n",
    "    title = pp.lemmetise_series(title)\n",
    "    title = \"\".join([char for char in title if char not in string.punctuation])\n",
    "    # remove stopwords\n",
    "    title = \" \".join([char for char in tokenizer.tokenize(title) if char not in stop_words ])\n",
    "    if len(title.split()) < length:\n",
    "         return None\n",
    "\n",
    "    return title\n",
    "# Convert to ascii\n",
    "# lower case \n",
    "# remove everything that is not printable.\n",
    "    \n",
    "def get_cleaned_headlines(url, length=3, tags=['h1', 'h2', 'h3']):\n",
    "    text = requests.get(url).text\n",
    "    return [clean_headlines(headline, length) for headline in get_headlines(text, tags=tags)]\n",
    "\n",
    "def convert_list_to_X(cleaned_headlines, pipeline):\n",
    "    # Convert list to a pandas sereios\n",
    "    series = pd.Series(cleaned_headlines, name='title')\n",
    "    X = pipeline.fit(X)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.Series(clean_headlines('clickbait is cancer', 2), name='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1                                  listen american life\n",
       "2                                   book review podcast\n",
       "3              congressional leader close stimulus deal\n",
       "4     us retail sale declined expected november rais...\n",
       "5     pandemic leaf military family seeking food ass...\n",
       "6                       appointee describe crushing cdc\n",
       "7                us seek ten million vaccine dos pfizer\n",
       "8                               full hospital icus near\n",
       "9     vaccination campaign nursing home face obstacl...\n",
       "10    health worker alaska serious allergic reaction...\n",
       "11    voter fraud misinformation ha subsided lie cor...\n",
       "12    biden introduces buttigieg pick lead transport...\n",
       "13    president elect joe biden considering diana ta...\n",
       "14       east coast prepares seasons first winter storm\n",
       "15                            russia win climate crisis\n",
       "16    cut emission zero us need make big change next...\n",
       "17               putting mask raised naomi osakas voice\n",
       "19                         yes young people dying covid\n",
       "20                               make think 2021 better\n",
       "21                        took mitch mcconnell six week\n",
       "22                       would known mom gone menopause\n",
       "24            reduce infection without closing business\n",
       "25                 kamala harris deserves important job\n",
       "26               50 pound bag flour gone sanity remains\n",
       "27              asian american coalition teach democrat\n",
       "28                       break democracy fatally weaken\n",
       "29             john le carre writer spy neighbor friend\n",
       "31                asked doe oreo keep releasing flavors\n",
       "32         red sox white sox blue sox manny still manny\n",
       "33                                   start making plans\n",
       "36                          site information navigation\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://www.nytimes.com/'\n",
    "cleaned_headlines = pd.Series(pp.get_cleaned_headlines(url), name='title')\n",
    "series = cleaned_headlines.dropna()\n",
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator BernoulliNB from version 0.21.3 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.21.3 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.21.3 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "f = open('./../../src/models/model1.pickle', 'rb')\n",
    "clf = pickle.load(f)\n",
    "f = open('./../../src/models/tfidf1.pickle', 'rb')\n",
    "tfidf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.Series(pp.clean_headlines('There’s A Sound That Apparently Only Teenagers Can Hear. Can You Hear It?', 2), name='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = test\n",
    "\n",
    "X_tfidf = tfidf.transform(X)\n",
    "predictions = clf.predict(X_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.sum()/predictions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>listen american life</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  title  target\n",
       "0  listen american life       1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = pd.Series(predictions, name='target')\n",
    "\n",
    "df = pd.DataFrame(list((zip(series, target))), columns=['title', 'target'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'timothee chalamets snl impression harry style weird thing'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_headlines(\"Timothée Chalamet’s “SNL” Impression Of Harry Styles Is Doing Weird Things To Me\", length=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JavaScript is not available.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_headlines(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'things from amazon that will make perfect gifts'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.remove_contractions(pp.lower_case(pp.remove_non_ascii_chars('Things From Amazon That’ll Make Perfect Gifts')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_headlines(response_text):\n",
    "    soup = BeautifulSoup(response_text, 'lxml')\n",
    "    headlines = soup.find_all(attrs={\"itemprop\": \"headline\"})\n",
    "    for headline in headlines:\n",
    "        print(headline.text)\n",
    "        \n",
    "def print_text(response_text):\n",
    "    soup = BeautifulSoup(response_text, 'lxml')\n",
    "    text = soup.find_all(\"h1\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.cnn.com/2020/12/13/health/us-coronavirus-sunday/index.html'\n",
    "response = requests.get(url)\n",
    "print_headlines(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h1 class=\"pg-headline\">Covid-19 vaccine en route to every state as health officials say they hope immunizations begin Monday</h1>]\n"
     ]
    }
   ],
   "source": [
    "print_text(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/tjh/Flatiron/capstone/bait-n-switch/notebooks/EDA/buffer.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32mbuffer.html\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0m__file__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'buffer.html'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'buffer.html'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/tjh/Flatiron/capstone/bait-n-switch/notebooks/EDA/buffer.html'"
     ]
    }
   ],
   "source": [
    "def replace(string, replacement_message, html):\n",
    "    response = requests.get(html)\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    nodes_to_censor = soup.findAll(text=re.compile(string))\n",
    "    for node in nodes_to_censor:\n",
    "        node.replaceWith(replacement_message)\n",
    "        print(node)\n",
    "\n",
    "__file__ = 'buffer.html'\n",
    "base = os.path.dirname(os.path.abspath(__file__))\n",
    "html = open(os.path.join(base, 'buffer.html'))\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "for i in soup.find('div', {\"id\":None}).findChildren():\n",
    "    i.replace_with('##')\n",
    "\n",
    "with open(\"example_modified.html\", \"wb\") as f_output:\n",
    "    f_output.write(soup.prettify(\"utf-8\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__ = 'buffer.html'\n",
    "base = os.path.dirname(os.path.abspath(__file__))\n",
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.abspath('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
