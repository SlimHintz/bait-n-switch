{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/tjh/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "# Custom modules \n",
    "from modules import preprocessing as pp\n",
    "from modules import graph\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note!\n",
    "\n",
    "Not all websites us h tags, some, a lot actually href links! This is somethjing to consider when it comes to designeing the actual web evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "# Extend stopwords (see analysis below)\n",
    "extension = {\n",
    "    'trumps',\n",
    "    'trump',\n",
    "    'obama',\n",
    "    'donald',\n",
    "    'new',\n",
    "    'u',\n",
    "    'tramp'\n",
    "}\n",
    "stop_words.update(extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first function just has to access a url and return a list of all headlines\n",
    "url = 'https://www.upworthy.com/netflix-fan-shares-hack-code-finding-shows'\n",
    "response = requests.get(url)\n",
    "\n",
    "def get_headlines(response_text, tags=['h1', 'h2', 'h3', 'h4']):\n",
    "    soup = BeautifulSoup(response_text, 'lxml')\n",
    "    headers = soup.find_all(tags)\n",
    "    return [header.text for header in headers]\n",
    "\n",
    "def clean_headlines(title, length):\n",
    "#     if len(title.split()) >= length:\n",
    "#         return None\n",
    "#     else:\n",
    "        # strip newline characters\n",
    "    title = title.replace(\"\\n\", \"\")\n",
    "    title = title.replace(\"\\t\", \"\")\n",
    "    title = pp.remove_non_ascii_chars(title)\n",
    "    title = pp.lower_case(title)\n",
    "    title = pp.remove_contractions(title)\n",
    "    title = pp.lemmetise_series(title)\n",
    "    title = \"\".join([char for char in title if char not in string.punctuation])\n",
    "    # remove stopwords\n",
    "    title = \" \".join([char for char in tokenizer.tokenize(title) if char not in stop_words ])\n",
    "    if len(title.split()) < length:\n",
    "         return None\n",
    "\n",
    "    return title\n",
    "# Convert to ascii\n",
    "# lower case \n",
    "# remove everything that is not printable.\n",
    "    \n",
    "def get_cleaned_headlines(url, length=3, tags=['h1', 'h2', 'h3']):\n",
    "    text = requests.get(url).text\n",
    "    return [clean_headlines(headline, length) for headline in get_headlines(text, tags=tags)]\n",
    "\n",
    "def convert_list_to_X(cleaned_headlines, pipeline):\n",
    "    # Convert list to a pandas sereios\n",
    "    series = pd.Series(cleaned_headlines, name='title')\n",
    "    X = pipeline.fit(X)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.Series(clean_headlines('clickbait is cancer', 2), name='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     netflix viewer cannot believe heard secret hac...\n",
       "1     megan wa trafficked year fbi rescued found ref...\n",
       "2     terminally ill washington elector openly wept ...\n",
       "3     megan wa trafficked year fbi rescued found ref...\n",
       "4     happens drinking 1 2 3 glass wine 19 viral pho...\n",
       "5     christmas gargoyle spark epic decoration war n...\n",
       "6     proud boy tore stomped set fire black churches...\n",
       "7     author whose son died 14 year ago ha word hope...\n",
       "8     woman found 4 yr olds fairy house spent 9 mont...\n",
       "9     gift give back shop upworthiest place gift giving\n",
       "10    uk change blood donation policy allowing gay b...\n",
       "11    dr bidens response sexist op ed suggesting dro...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://www.upworthy.com/netflix-fan-shares-hack-code-finding-shows'\n",
    "cleaned_headlines = pd.Series(pp.get_cleaned_headlines(url), name='title')\n",
    "series = cleaned_headlines.dropna()\n",
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./../../src/models/model1.1.pickle', 'rb')\n",
    "clf = pickle.load(f)\n",
    "f = open('./../../src/models/tfidf1.1.pickle', 'rb')\n",
    "tfidf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.Series(pp.clean_headlines('There’s A Sound That Apparently Only Teenagers Can Hear. Can You Hear It?', 2), name='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = series\n",
    "\n",
    "X_tfidf = tfidf.transform(X)\n",
    "predictions = clf.predict(X_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4166666666666667"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.sum()/predictions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>netflix viewer cannot believe heard secret hac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>megan wa trafficked year fbi rescued found ref...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>terminally ill washington elector openly wept ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>megan wa trafficked year fbi rescued found ref...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>happens drinking 1 2 3 glass wine 19 viral pho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>christmas gargoyle spark epic decoration war n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>proud boy tore stomped set fire black churches...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>author whose son died 14 year ago ha word hope...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>woman found 4 yr olds fairy house spent 9 mont...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gift give back shop upworthiest place gift giving</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>uk change blood donation policy allowing gay b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dr bidens response sexist op ed suggesting dro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  target\n",
       "0   netflix viewer cannot believe heard secret hac...       1\n",
       "1   megan wa trafficked year fbi rescued found ref...       0\n",
       "2   terminally ill washington elector openly wept ...       0\n",
       "3   megan wa trafficked year fbi rescued found ref...       0\n",
       "4   happens drinking 1 2 3 glass wine 19 viral pho...       1\n",
       "5   christmas gargoyle spark epic decoration war n...       0\n",
       "6   proud boy tore stomped set fire black churches...       0\n",
       "7   author whose son died 14 year ago ha word hope...       1\n",
       "8   woman found 4 yr olds fairy house spent 9 mont...       1\n",
       "9   gift give back shop upworthiest place gift giving       1\n",
       "10  uk change blood donation policy allowing gay b...       0\n",
       "11  dr bidens response sexist op ed suggesting dro...       0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = pd.Series(predictions, name='target')\n",
    "\n",
    "df = pd.DataFrame(list((zip(series, target))), columns=['title', 'target'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'timothee chalamets snl impression harry style weird thing'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_headlines(\"Timothée Chalamet’s “SNL” Impression Of Harry Styles Is Doing Weird Things To Me\", length=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This browser is no longer supported.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_headlines(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'things from amazon that will make perfect gifts'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.remove_contractions(pp.lower_case(pp.remove_non_ascii_chars('Things From Amazon That’ll Make Perfect Gifts')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_headlines(response_text):\n",
    "    soup = BeautifulSoup(response_text, 'lxml')\n",
    "    headlines = soup.find_all(attrs={\"itemprop\": \"headline\"})\n",
    "    for headline in headlines:\n",
    "        print(headline.text)\n",
    "        \n",
    "def print_text(response_text):\n",
    "    soup = BeautifulSoup(response_text, 'lxml')\n",
    "    text = soup.find_all(\"h1\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.cnn.com/2020/12/13/health/us-coronavirus-sunday/index.html'\n",
    "response = requests.get(url)\n",
    "print_headlines(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h1 class=\"pg-headline\">Covid-19 vaccine en route to every state as health officials say they hope immunizations begin Monday</h1>]\n"
     ]
    }
   ],
   "source": [
    "print_text(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/tjh/Flatiron/capstone/bait-n-switch/notebooks/EDA/buffer.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32mbuffer.html\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0m__file__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'buffer.html'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'buffer.html'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/tjh/Flatiron/capstone/bait-n-switch/notebooks/EDA/buffer.html'"
     ]
    }
   ],
   "source": [
    "def replace(string, replacement_message, html):\n",
    "    response = requests.get(html)\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    nodes_to_censor = soup.findAll(text=re.compile(string))\n",
    "    for node in nodes_to_censor:\n",
    "        node.replaceWith(replacement_message)\n",
    "        print(node)\n",
    "\n",
    "__file__ = 'buffer.html'\n",
    "base = os.path.dirname(os.path.abspath(__file__))\n",
    "html = open(os.path.join(base, 'buffer.html'))\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "for i in soup.find('div', {\"id\":None}).findChildren():\n",
    "    i.replace_with('##')\n",
    "\n",
    "with open(\"example_modified.html\", \"wb\") as f_output:\n",
    "    f_output.write(soup.prettify(\"utf-8\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__ = 'buffer.html'\n",
    "base = os.path.dirname(os.path.abspath(__file__))\n",
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.abspath('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
