{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/TjH/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "# Custom modules \n",
    "from modules import preprocessing as pp\n",
    "from modules import graph\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "# Extend stopwords (see analysis below)\n",
    "extension = {\n",
    "    'trumps',\n",
    "    'trump',\n",
    "    'obama',\n",
    "    'donald',\n",
    "    'new',\n",
    "    'u',\n",
    "    'tramp'\n",
    "}\n",
    "stop_words.update(extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first function just has to access a url and return a list of all headlines\n",
    "url = 'https://twitter.com/home'\n",
    "response = requests.get(url)\n",
    "\n",
    "def get_headlines(response_text, tags=['h1', 'h2', 'h3', 'h4']):\n",
    "    soup = BeautifulSoup(response_text, 'lxml')\n",
    "    headers = soup.find_all(tags)\n",
    "    return [header.text for header in headers]\n",
    "\n",
    "def clean_headlines(title, length):\n",
    "#     if len(title.split()) >= length:\n",
    "#         return None\n",
    "#     else:\n",
    "        # strip newline characters\n",
    "    title = title.replace(\"\\n\", \"\")\n",
    "    title = title.replace(\"\\t\", \"\")\n",
    "    title = pp.remove_non_ascii_chars(title)\n",
    "    title = pp.lower_case(title)\n",
    "    title = pp.remove_contractions(title)\n",
    "    title = pp.lemmetise_series(title)\n",
    "    title = \"\".join([char for char in title if char not in string.punctuation])\n",
    "    # remove stopwords\n",
    "    title = \" \".join([char for char in tokenizer.tokenize(title) if char not in stop_words ])\n",
    "    if len(title.split()) < length:\n",
    "         return None\n",
    "\n",
    "    return title\n",
    "# Convert to ascii\n",
    "# lower case \n",
    "# remove everything that is not printable.\n",
    "    \n",
    "def get_cleaned_headlines(url, length=3, tags=['h1', 'h2', 'h3']):\n",
    "    text = requests.get(url).text\n",
    "    return [clean_headlines(headline, length) for headline in get_headlines(text, tags=tags)]\n",
    "\n",
    "def convert_list_to_X(cleaned_headlines, pipeline):\n",
    "    # Convert list to a pandas sereios\n",
    "    series = pd.Series(cleaned_headlines, name='title')\n",
    "    X = pipeline.fit(X)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.Series(clean_headlines('clickbait is cancer', 2), name='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1                                    neediest case fund\n",
       "2                             got confidential news tip\n",
       "3     electoral college track formalize bidens victo...\n",
       "4     key swing state affirm biden win despite pressure\n",
       "5     long honor serving electoral college voter als...\n",
       "6     us virus death toll cross 300000 vaccination b...\n",
       "7           day hope reminder viruss devastating impact\n",
       "8                 company require employee take vaccine\n",
       "9     political infighting haphazard planning turned...\n",
       "10                       2020 year sport everybody lost\n",
       "11              restaurant thought 2020 could get worse\n",
       "12    one study pregnant woman covid 19 experienced ...\n",
       "13            american stuck home trade china roar back\n",
       "14    john le carre master spy novel real action wa ...\n",
       "15          life defied alzheimers death brain may show\n",
       "17                    people actually pretty great year\n",
       "19                          get herd immunity fake news\n",
       "20             autonomous vehicle take another big leap\n",
       "21             pound pound taiwan important place world\n",
       "22                          ha never believed democracy\n",
       "23                    georgias secretary state standing\n",
       "24                                  getting vote matter\n",
       "25                                book really easy wrap\n",
       "26                      test lifetimes let stop failing\n",
       "27                         republican embraced nihilism\n",
       "29               indonesia blurred boundary living dead\n",
       "30    agents mistake cost nba player 3 million paid ...\n",
       "31          america learn south africa national healing\n",
       "34                          site information navigation\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://www.nytimes.com/'\n",
    "cleaned_headlines = pd.Series(pp.get_cleaned_headlines(url), name='title')\n",
    "series = cleaned_headlines.dropna()\n",
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./../../src/models/model1.pickle', 'rb')\n",
    "clf = pickle.load(f)\n",
    "f = open('./../../src/models/tfidf1.pickle', 'rb')\n",
    "tfidf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.Series(pp.clean_headlines('clickbait is cancer here are 25 what you can do to change it', 2), name='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = series\n",
    "\n",
    "X_tfidf = tfidf.transform(X)\n",
    "predictions = clf.predict(X_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3448275862068966"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.sum()/predictions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1                             got confidential news tip\n",
       "6           day hope reminder viruss devastating impact\n",
       "9                        2020 year sport everybody lost\n",
       "10              restaurant thought 2020 could get worse\n",
       "11    one study pregnant woman covid 19 experienced ...\n",
       "13    john le carre master spy novel real action wa ...\n",
       "16                    people actually pretty great year\n",
       "18                                book really easy wrap\n",
       "19                          ha never believed democracy\n",
       "24                      test lifetimes let stop failing\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = pd.Series(predictions, name='target')\n",
    "\n",
    "df = pd.DataFrame(list((zip(series, target))), columns=['title', 'target'])\n",
    "df[df.target == 1].title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'timothee chalamets snl impression harry style weird thing'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_headlines(\"Timothée Chalamet’s “SNL” Impression Of Harry Styles Is Doing Weird Things To Me\", length=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Your Weekend Briefing',\n",
       " 'Listen to ‘The Sunday Read’',\n",
       " 'The Neediest Cases Fund',\n",
       " 'White House Staff Will Be Among the First in the U.S. to Get Vaccinated',\n",
       " ' ',\n",
       " '\\n\\n\\t\\t\\t\\t\\t\\t\\t\\tNew Reported Cases in the U.S.\\n\\t\\t\\t\\t\\t\\t\\t\\n',\n",
       " '\\n\\n\\t\\t\\t\\t\\t\\t\\t\\tHow New Cases Are Changing by Day\\n\\t\\t\\t\\t\\t\\t\\t\\n',\n",
       " 'After the Students Came Back, Deaths Rose in College Towns',\n",
       " 'These health pass apps could help reopen businesses, but could also exclude people from travel and workplaces.',\n",
       " '2020 Was Especially Deadly. Covid Wasn’t the Only Culprit.',\n",
       " 'Germany is going into lockdown ahead of Christmas, closing stores and schools and restricting meetings.',\n",
       " 'Despite early worries, the threat of dueling flu and coronavirus outbreaks may be waning.',\n",
       " 'Tracking the Coronavirus ›',\n",
       " 'Where cases per capita are\\n\\t\\thighest',\n",
       " 'U.S. hot spots ›',\n",
       " 'College cases ›',\n",
       " 'Worldwide ›',\n",
       " 'Other trackers:\\nChoose your own places to track',\n",
       " 'Other trackers:',\n",
       " 'U.S. hot spots ›',\n",
       " 'Worldwide ›',\n",
       " 'Vaccine tracker ›',\n",
       " 'Other trackers:\\n',\n",
       " 'Other trackers:',\n",
       " 'U.S. hot spots ›',\n",
       " 'Worldwide ›',\n",
       " 'Vaccine tracker ›',\n",
       " 'Other trackers:\\n',\n",
       " 'Other trackers:',\n",
       " 'Man Is Arrested in Stabbing at D.C. Election Protest',\n",
       " 'Trump Allies Eye Long-Shot Election Reversal in Congress, Testing Pence',\n",
       " 'Republicans found themselves speechless after President Trump suffered a stinging defeat at the Supreme Court.',\n",
       " 'After Another Court Loss in Wisconsin, Trump Says Fight Is ‘Not Over’',\n",
       " 'As Biden Prepares to Take Office, a New Rush at the Border',\n",
       " 'What to know about tomorrow’s Electoral College vote.',\n",
       " 'She Stalked Her Daughter’s Killers Across Mexico, One by One',\n",
       " 'New York Has a Cameo in ‘The Crown.’ Here’s What Really Happened.',\n",
       " 'The Weekender: Obama Opens Up About Writing ‘A Promised Land’',\n",
       " 'Did you follow the headlines this week? Try our news quiz.',\n",
       " 'Opinion',\n",
       " 'Why Getting the Most Votes Matters',\n",
       " 'This Is the Test of Our Lifetimes. Let’s Stop Failing It.',\n",
       " 'The Texas Lawsuit and the Age of Dreampolitik',\n",
       " 'The Republicans Who Embraced Nihilism',\n",
       " '‘These Girls Are Being Cut and Married in Droves’',\n",
       " 'Where I Find Hope',\n",
       " 'The Virus, the Bats and Us',\n",
       " 'I Achieved My Wildest Dreams. Then Depression Hit.',\n",
       " 'What Really Saved the Republic From Trump?',\n",
       " 'We Must Do More to Stop Dangerous Doctors in a Pandemic',\n",
       " 'It’s Time to Scare People About Covid',\n",
       " 'Editors’ Picks',\n",
       " 'N.F.L. Playoff Picture: Each Team’s Chances',\n",
       " 'Don’t Want to Spring for a Hot Tub? Try a Stock Tank Pool',\n",
       " '‘Saturday Night Live’ Sends Up Covid-19 Vaccine Rollout',\n",
       " 'Advertisement',\n",
       " 'Site Index',\n",
       " 'Site Information Navigation']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_headlines(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'things from amazon that will make perfect gifts'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.remove_contractions(pp.lower_case(pp.remove_non_ascii_chars('Things From Amazon That’ll Make Perfect Gifts')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_headlines(response_text):\n",
    "    soup = BeautifulSoup(response_text, 'lxml')\n",
    "    headlines = soup.find_all(attrs={\"itemprop\": \"headline\"})\n",
    "    for headline in headlines:\n",
    "        print(headline.text)\n",
    "        \n",
    "def print_text(response_text):\n",
    "    soup = BeautifulSoup(response_text, 'lxml')\n",
    "    text = soup.find_all(\"h1\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.cnn.com/2020/12/13/health/us-coronavirus-sunday/index.html'\n",
    "response = requests.get(url)\n",
    "print_headlines(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h1 class=\"pg-headline\">CDC officially allows coronavirus vaccine to be administered as shipments begin in US</h1>]\n"
     ]
    }
   ],
   "source": [
    "print_text(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'findChildren'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/Users/TjH/Flatiron/capstone/bait-n-switch/notebooks/EDA/buffer.html\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindChildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'##'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'findChildren'"
     ]
    }
   ],
   "source": [
    "def replace(string, replacement_message, html):\n",
    "    response = requests.get(html)\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    nodes_to_censor = soup.findAll(text=re.compile(string))\n",
    "    for node in nodes_to_censor:\n",
    "        node.replaceWith(replacement_message)\n",
    "        print(node)\n",
    "\n",
    "__file__ = 'buffer.html'\n",
    "base = os.path.dirname(os.path.abspath(__file__))\n",
    "html = open(os.path.join(base, 'buffer.html'))\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "for i in soup.find('div', {\"id\":None}).findChildren():\n",
    "    i.replace_with('##')\n",
    "\n",
    "with open(\"example_modified.html\", \"wb\") as f_output:\n",
    "    f_output.write(soup.prettify(\"utf-8\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__ = 'buffer.html'\n",
    "base = os.path.dirname(os.path.abspath(__file__))\n",
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.abspath('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
